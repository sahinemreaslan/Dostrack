DATA:
  MAX_SAMPLE_INTERVAL: 200
  MEAN:
  - 0.485
  - 0.456
  - 0.406
  SEARCH:
    CENTER_JITTER: 3
    FACTOR: 4.0
    SCALE_JITTER: 0.25
    SIZE: 256
    NUMBER: 1
  STD:
  - 0.229
  - 0.224
  - 0.225
  TEMPLATE:
    CENTER_JITTER: 0
    FACTOR: 2.0
    SCALE_JITTER: 0
    SIZE: 128

  TRAIN:
    DATASETS_NAME:
    - LASOT
    DATASETS_RATIO:
    - 1
    SAMPLE_PER_EPOCH: 60000
  VAL:
    DATASETS_NAME:
    - LASOT
    DATASETS_RATIO:
    - 1
    SAMPLE_PER_EPOCH: 10000

MODEL:
  PRETRAIN_FILE: "dinov3_vits16_pretrain.pth"
  EXTRA_MERGER: False
  RETURN_INTER: False
  BACKBONE:
    TYPE: dinov3_vits16  # Small model with 16x16 patches
    STRIDE: 16
    FROZEN: False  # Enable LoRA fine-tuning (not frozen)
    USE_LORA: True  # Use LoRA adaptation
    LORA_RANK: 8    # LoRA rank (low-rank dimension)
    LORA_ALPHA: 16  # LoRA scaling factor (alpha/rank = 2.0 scaling)
    # LoRA dropout is set to 0.1 by default in LoRAConfig
    # NO CE configuration - remove CE entirely for simplicity
  HEAD:
    TYPE: TrulyDynamicCenterPredictor
    NUM_CHANNELS: 256

TRAIN:
  BACKBONE_MULTIPLIER: 0.01  # Low learning rate for LoRA parameters (1% of head LR)
  DROP_PATH_RATE: 0.1  # Stochastic depth for regularization
  # NO CE epochs - remove CE entirely
  BATCH_SIZE: 128  # Reduced from 128 due to gradients in backbone
  EPOCH: 300
  GIOU_WEIGHT: 2.0
  L1_WEIGHT: 5.0
  GRAD_CLIP_NORM: 0.1
  LR: 0.0004  # Lower learning rate for LoRA fine-tuning
  LR_DROP_EPOCH: 240
  NUM_WORKER: 10
  OPTIMIZER: ADAMW
  PRINT_INTERVAL: 50
  SCHEDULER:
    TYPE: step
    DECAY_RATE: 0.1
  VAL_EPOCH_INTERVAL: 20
  WEIGHT_DECAY: 0.0001
  AMP: False  # Mixed precision can be enabled for speed

TEST:
  EPOCH: 300
  SEARCH_FACTOR: 4.0
  SEARCH_SIZE: 256
  TEMPLATE_FACTOR: 2.0
  TEMPLATE_SIZE: 128
